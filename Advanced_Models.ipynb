{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "## display all the columns\n",
    "pd.options.display.max_columns = 500\n",
    "pd.options.display.max_rows = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "##combine all the features into one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieDF = pd.read_csv('movie_df.csv')\n",
    "twitterDF = pd.read_csv('twitter_bag_final.csv')\n",
    "sentimentDF = pd.read_csv('sentimentresult.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.merge(twitterDF,sentimentDF,how='inner',on='Movie')\n",
    "merged_data = pd.merge(df1,movieDF,how='inner',left_on='Movie',right_on='movie_title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aspect_ratio              50\n",
       "budget                    30\n",
       "runtime                    1\n",
       "facenumber_in_poster       5\n",
       "num_critic_for_reviews     2\n",
       "gross                     76\n",
       "overview                   1\n",
       "color                      4\n",
       "content_rating            22\n",
       "language                   1\n",
       "plot_keywords             25\n",
       "dtype: int64"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 1 check missing value\n",
    "# Number of Nans\n",
    "nan_list = merged_data.count() \n",
    "len(merged_data) - nan_list[nan_list < len(merged_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average(data):\n",
    "    avg = np.sum(data)/data.count()\n",
    "    return avg\n",
    "\n",
    "def get_common(data):\n",
    "    mod = data.mode()\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step2 deal with missing value --- this is temporary method to do that\n",
    "# duration num_critic_for_reviews, actor_2_facebook_likes,\n",
    "# actor_3_facebook_likes,budget_x,runtime,duration,num_user_for_reviews\n",
    "# fill with average\n",
    "merged_data['duration'] = merged_data['duration'].fillna(get_average(merged_data['duration']))\n",
    "merged_data['actor_2_facebook_likes'] = merged_data['actor_2_facebook_likes'].fillna(get_average(merged_data['actor_2_facebook_likes']))\n",
    "merged_data['budget'] = merged_data['budget'].fillna(get_average(merged_data['budget']))\n",
    "merged_data['runtime'] = merged_data['runtime'].fillna(get_average(merged_data['runtime']))\n",
    "merged_data['duration'] = merged_data['duration'].fillna(get_average(merged_data['duration']))\n",
    "merged_data['num_user_for_reviews'] = merged_data['num_user_for_reviews'].fillna(get_average(merged_data['num_user_for_reviews']))\n",
    "merged_data['actor_3_facebook_likes'] = merged_data['actor_3_facebook_likes'].fillna(get_average(merged_data['actor_3_facebook_likes']))\n",
    "merged_data['num_critic_for_reviews'] = merged_data['num_critic_for_reviews'].fillna(get_average(merged_data['num_critic_for_reviews']))\n",
    "\n",
    "# facenumber_in_poster,language,color with the most common one\n",
    "merged_data['facenumber_in_poster'] = merged_data['facenumber_in_poster'].fillna(get_common(merged_data['facenumber_in_poster'])[0])\n",
    "merged_data['language'] = merged_data['language'].fillna(get_common(merged_data['language'])[0])\n",
    "merged_data['color'] = merged_data['color'].fillna(get_common(merged_data['color'])[0])\n",
    "\n",
    "# delete: gross, plot_keywords, content_rating, home_page, tagline, aspect_ratio fill\n",
    "merged_data = merged_data.drop(columns = ['gross', 'plot_keywords', 'content_rating', 'aspect_ratio'\\\n",
    "                                         ,'actor_2_name','overview','actor_3_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 1 check missing value\n",
    "# Number of Nans\n",
    "nan_list = merged_data.count() \n",
    "len(merged_data) - nan_list[nan_list < len(merged_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0_x', 'Unnamed: 0.1', 'content', 'Movie', 'Startdate',\n",
      "       'Enddate', 'age', 'bad', 'best', 'big',\n",
      "       ...\n",
      "       'has_homepage', 'has_tagline', 'tagline_word_count', 'release_weekday',\n",
      "       'release_month', 'release_year', 'keywords_count',\n",
      "       'production_companies_count', 'production_countries_count',\n",
      "       'spoken_languages_count'],\n",
      "      dtype='object', length=159)\n"
     ]
    }
   ],
   "source": [
    "print(merged_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop features with sentence or words for baseline model\n",
    "#also delete features which we cannot get before the movie shows\n",
    "merged_data = merged_data.drop(columns = ['director_name','actor_1_name', 'movie_title','num_voted_users','movie_imdb_link','title_year',\\\n",
    "                           'imdb_score','id', 'keywords','original_language','original_title',\\\n",
    "                           'production_companies', 'production_countries', 'release_date','spoken_languages', 'title',\n",
    "                            'vote_average', 'vote_count','Unnamed: 0_x','Unnamed: 0_y','content','Movie','Startdate','Enddate','text','Unnamed: 0'\\\n",
    "                                         ,'Unnamed: 0.1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4 mapping and one-hot encoding\n",
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(merged_data.color)\n",
    "merged_data.color = le.fit_transform(merged_data.color)\n",
    "\n",
    "le1 = preprocessing.LabelEncoder()\n",
    "le1.fit(merged_data.genres)\n",
    "merged_data.genres = le1.fit_transform(merged_data.genres)\n",
    "\n",
    "le2 = preprocessing.LabelEncoder()\n",
    "le2.fit(merged_data.language)\n",
    "merged_data.language = le2.fit_transform(merged_data.language)\n",
    "\n",
    "le3 = preprocessing.LabelEncoder()\n",
    "le3.fit(merged_data.country)\n",
    "merged_data.country = le3.fit_transform(merged_data.country)\n",
    "\n",
    "le4 = preprocessing.LabelEncoder()\n",
    "le4.fit(merged_data.release_year)\n",
    "merged_data.release_year= le4.fit_transform(merged_data.release_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(389, 132)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The features whose variance is below the threshold are:\n",
      " []\n"
     ]
    }
   ],
   "source": [
    "#feature selection\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "selector = VarianceThreshold(0.01)\n",
    "selector.fit(merged_data)\n",
    "reduced_features = [merged_data.columns[i] for i in range(len(selector.get_support(indices=False))) if selector.get_support(indices=False)[i] == False]\n",
    "merged_data = merged_data.drop(columns = reduced_features)\n",
    "print('The features whose variance is below the threshold are:\\n', reduced_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets , linear_model\n",
    "from sklearn.metrics import mean_squared_error , r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = merged_data.drop(columns=['revenue'])\n",
    "y = merged_data['revenue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept_:67329490.793\n",
      "Mean squared error: 27503192631937856.000\n",
      "Variance score: 0.665\n",
      "score: 0.665\n"
     ]
    }
   ],
   "source": [
    "# Step 5 Trying a baseline model\n",
    "X_train , X_test , y_train ,y_test = train_test_split(X,y,test_size=0.2 ,random_state=55)\n",
    "LR = linear_model.LinearRegression()\n",
    "LR.fit(X_train,y_train)\n",
    "print('intercept_:%.3f' % LR.intercept_)\n",
    "#print('coef_:%.3f' % LR.coef_)\n",
    "print('Mean squared error: %.3f' % mean_squared_error(y_test,LR.predict(X_test)))\n",
    "##((y_test-LR.predict(X_test))**2).mean()\n",
    "print('Variance score: %.3f' % r2_score(y_test,LR.predict(X_test)))\n",
    "#1-((y_test-LR.predict(X_test))**2).sum()/((y_test - y_test.mean())**2).sum()\n",
    "print('score: %.3f' % LR.score(X_test,y_test))\n",
    "#plt.scatter(X_test , y_test ,color ='green')\n",
    "#plt.plot(X_test ,LR.predict(X_test) ,color='red',linewidth =3)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 95363906588894608.000\n",
      "Variance score: -0.162\n",
      "score: -0.162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhuwe\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Advanced Model\n",
    "from sklearn.svm import SVR\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "transformer = Normalizer().fit(X)\n",
    "X1 = transformer.transform(X)\n",
    "pca = PCA(n_components=50)\n",
    "X2 = pca.fit_transform(X1)\n",
    "X_train , X_test , y_train ,y_test = train_test_split(X1,y,test_size=0.2 ,random_state=55)\n",
    "\n",
    "svr = SVR(kernel='poly', C=1000, degree=10)\n",
    "svr.fit(X_train, y_train) \n",
    "\n",
    "#print('coef_:%.3f' % LR.coef_)\n",
    "print('Mean squared error: %.3f' % mean_squared_error(y_test,svr.predict(X_test)))\n",
    "##((y_test-LR.predict(X_test))**2).mean()\n",
    "print('Variance score: %.3f' % r2_score(y_test,svr.predict(X_test)))\n",
    "#1-((y_test-LR.predict(X_test))**2).sum()/((y_test - y_test.mean())**2).sum()\n",
    "print('score: %.3f' % svr.score(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=50)\n",
    "X2 = pca.fit_transform(X)\n",
    "X_train , X_test , y_train ,y_test = train_test_split(X2,y,test_size=0.2 ,random_state=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 27402006390378700.000\n",
      "Variance score: 0.666\n",
      "score: 0.666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "RF = RandomForestRegressor(max_depth=10, random_state=0,\n",
    "                          n_estimators=100)\n",
    "RF.fit(X_train, y_train) \n",
    "#print('coef_:%.3f' % LR.coef_)\n",
    "print('Mean squared error: %.3f' % mean_squared_error(y_test,RF.predict(X_test)))\n",
    "##((y_test-LR.predict(X_test))**2).mean()\n",
    "print('Variance score: %.3f' % r2_score(y_test,RF.predict(X_test)))\n",
    "#1-((y_test-LR.predict(X_test))**2).sum()/((y_test - y_test.mean())**2).sum()\n",
    "print('score: %.3f' % RF.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 29081076344593084.000\n",
      "Variance score: 0.646\n",
      "score: 0.646\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "adb = AdaBoostRegressor(random_state=0, n_estimators=1000)\n",
    "adb.fit(X_train, y_train)  \n",
    "adb.score(X_test, y_test) \n",
    "#print('coef_:%.3f' % LR.coef_)\n",
    "print('Mean squared error: %.3f' % mean_squared_error(y_test,adb.predict(X_test)))\n",
    "##((y_test-LR.predict(X_test))**2).mean()\n",
    "print('Variance score: %.3f' % r2_score(y_test,adb.predict(X_test)))\n",
    "#1-((y_test-LR.predict(X_test))**2).sum()/((y_test - y_test.mean())**2).sum()\n",
    "print('score: %.3f' % adb.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 19848138293722380.000\n",
      "Variance score: 0.758\n",
      "score: 0.758\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbm = GradientBoostingRegressor(random_state=0, n_estimators=1000)\n",
    "gbm.fit(X_train, y_train)  \n",
    "gbm.score(X_test, y_test) \n",
    "#print('coef_:%.3f' % LR.coef_)\n",
    "print('Mean squared error: %.3f' % mean_squared_error(y_test,gbm.predict(X_test)))\n",
    "##((y_test-LR.predict(X_test))**2).mean()\n",
    "print('Variance score: %.3f' % r2_score(y_test,gbm.predict(X_test)))\n",
    "#1-((y_test-LR.predict(X_test))**2).sum()/((y_test - y_test.mean())**2).sum()\n",
    "print('score: %.3f' % gbm.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
